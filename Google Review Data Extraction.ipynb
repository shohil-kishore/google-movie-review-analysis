{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shrek.html\n",
      "soundOfMusic.html\n",
      "loveAcutally.html\n",
      "noCountryForOldMen.html\n",
      "signs.html\n",
      "itsAWonderfulLife.html\n",
      "bladeRunner.html\n",
      "polarExpress.html\n",
      "gladiator.html\n",
      "theShining.html\n",
      "kingKong.html\n",
      "lesMiserables.html\n",
      "spiderman.html\n",
      "theDeparted.html\n",
      "phantomOfTheOpera.html\n",
      "brokebackMountain.html\n",
      "fridayThe13th.html\n",
      "theIncredibles.html\n",
      "pansLabyrinth.html\n",
      "lordOfTheRingsReturnOfTheKing.html\n",
      "returnOfTheJedi.html\n",
      "300.html\n",
      "firefly.html\n",
      "sinCity.html\n",
      "lostInTranslation.html\n",
      "piratesOfTheCaribbean.html\n",
      "downtonAbbey.html\n",
      "memento.html\n",
      "planetOfTheApes.html\n",
      "halloween.html\n",
      "theDarkNight.html\n",
      "jurassicPark.html\n",
      "harryPotterAndThePrisonerOfAzkaban.html\n",
      "independenceDay.html\n",
      "goneWithTheWind.html\n",
      "pulpFiction.html\n",
      "starWarsEpisode1.html\n",
      "harryPotterAndThePhilosophersStone.html\n",
      "sleepHollow.html\n",
      "amelie.html\n",
      "pearlHarbor.html\n",
      "24.html\n",
      "theMatrix.html\n",
      "indianJonesAndTheTempleOfDoom.html\n",
      "troy.html\n",
      "batmanBegins.html\n",
      "theExorcist.html\n",
      "napoleonDynamite.html\n",
      "aIArtificialIntelligence.html\n",
      "starWarsEpisodeIV.html\n",
      "supermanReturns.html\n",
      "vanHelsing.html\n",
      "schindlersList.html\n",
      "spiderMan2.html\n",
      "theDayAfterTomorrow.html\n",
      "Alien.html\n",
      "fahrentheit911.html\n",
      "goodFellas.html\n",
      "riddick.html\n",
      "fightClub.html\n",
      "passionOfTheChrist.html\n",
      "starWarsEpisode2.html\n",
      "theShawshankRedemption.html\n",
      "theSixthSense.html\n",
      "prideAndPrejudice.html\n",
      "terminator3.html\n",
      "savingPrivateRyan.html\n",
      "lordOfTheRIngsFellowshipOfTheRing.html\n",
      "bandOfBrothers.html\n",
      "starWarsEpisode3.html\n",
      "killBill.html\n",
      "2001SpaceOdyssey.html\n",
      "theLastSamuria.html\n",
      "equilibrium.html\n",
      "vanHelsingSeries.html\n",
      "titanic.html\n",
      "galaxyQuest.html\n",
      "theGoonies.html\n",
      "americanBeauty.html\n",
      "warOfTheWorlds.html\n",
      "crash.html\n",
      "nationalLampoonsChristmasVacation.html\n",
      "jaws.html\n",
      "serenity.html\n",
      "theWizzardOfOz.html\n",
      "forrestGump.html\n",
      "theChroniclesOfNarnia.html\n",
      "wallE.html\n",
      "starWarsEpisodeV.html\n",
      "casablanca.html\n",
      "vForVendetta.html\n",
      "ironMan.html\n",
      "aChristmasStory.html\n",
      "transformers.html\n",
      "theVillage.html\n",
      "starTrekOriginalSeries.html\n",
      "terminator.html\n",
      "moulinRouge.html\n",
      "findingNemo.html\n",
      "theGodfather.html\n",
      "iAmLegend.html\n",
      "monsters.html\n"
     ]
    }
   ],
   "source": [
    "# List all HTML files given by Staff Mobility Service.\n",
    "for filename in os.listdir(os.getcwd() + \"/HTML Files\"):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each file, create a DF, open the file and use BeautifulSoup to extract data and import to individual rows (df.loc[i]). Finally, write to CSV file.\n",
    "for file in os.listdir(os.getcwd() + \"/HTML Files\"):\n",
    "    filePath = './HTML Files/' + file\n",
    "    csvFileName = file + \".csv\"\n",
    "    \n",
    "    # Setup DataFrame to add rows later. \n",
    "    df = pd.DataFrame(columns = ['Review', 'Helpfulness', 'Reviewer Name', 'Movie Title', 'Data Source'])\n",
    "    html = open(filePath, \"r\", encoding='utf-8')\n",
    "    html = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # Finding outside containers of each review (including helpfulness)\n",
    "    containers = html.find_all(\"div\", {\"jsname\":\"HeNW9\"})\n",
    "    \n",
    "    # Extracts useful info, text removes outer tags.\n",
    "    for i in range(len(containers)):\n",
    "        try: \n",
    "            review = str(containers[i].find(\"pre\", {\"jsname\":\"QUIPvd\"}).text) \n",
    "            helpfulness = str(containers[i].find(\"div\", {\"class\":\"gjvCCd\"}).text)\n",
    "            userName = str(containers[i].find(\"div\", {\"class\":\"GyeGK\"}).text)\n",
    "            movieName = file\n",
    "            dataSource = \"Google\"\n",
    "            df.loc[i] = review, helpfulness, userName, movieName, dataSource\n",
    "        except AttributeError: \n",
    "            pass\n",
    "        \n",
    "    df.to_csv(csvFileName, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below is not run sequentially to the code above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to combine all CSV files generated from the code above. \n",
    "import glob, os\n",
    "import pandas as pd\n",
    "path = r'/Users/shohilkishore/Desktop/Work/Gabrielle Peko/Google Data/Data Files'                     # use your path\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "\n",
    "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "concat_df   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "concat_df['Helpfulness'] = concat_df['Helpfulness'].str.replace('Was this review helpful to you\\?', '0')\n",
    "concat_df['Helpfulness'] = concat_df['Helpfulness'].str.replace('1 person found this helpful.', '1')\n",
    "for i in range(200):\n",
    "        toReplace = str(i) + \" people found this helpful.\"\n",
    "        concat_df['Helpfulness'] = concat_df['Helpfulness'].str.replace(toReplace, str(i))\n",
    "\n",
    "concat_df['Helpfulness'] = concat_df['Helpfulness'].str.replace(' ', '')\n",
    "concat_df['Helpfulness'] = concat_df['Helpfulness'].replace('\\n','', regex=True)\n",
    "# concat_df.to_csv(\"Combined Google Reviews.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.to_csv(\"Combined Reviews - Google.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
